{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "from time import time\n",
    "from typing import Iterable, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(\"train.csv\") or not os.path.isfile(\"test.csv\"):\n",
    "    print(\"Downloading data...\")\n",
    "    train = pd.read_csv(\"https://www.csie.ntu.edu.tw/~htlin/course/ml20fall/hw6/hw6_train.dat\", sep=\" \", header=None)\n",
    "    test = pd.read_csv(\"https://www.csie.ntu.edu.tw/~htlin/course/ml20fall/hw6/hw6_test.dat\", sep=\" \", header=None)\n",
    "\n",
    "    train.to_csv(\"train.csv\", index=False)\n",
    "    test.to_csv(\"test.csv\", index=False)\n",
    "    print(\"Completed\")\n",
    "else:\n",
    "    train = pd.read_csv(\"train.csv\")\n",
    "    test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, threshold, feature, value=None):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.value = value\n",
    "        self.feature = feature\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "def tree_predict(tree: TreeNode, data: List[float]) -> float:\n",
    "    while (not tree.value):\n",
    "        if data[int(tree.feature)] >= tree.threshold:\n",
    "            tree = tree.right\n",
    "        else:\n",
    "            tree = tree.left\n",
    "\n",
    "    return tree.value\n",
    "\n",
    "\n",
    "def learn_tree(data: pd.DataFrame) -> List[List]:\n",
    "    if is_pure(data):\n",
    "        return TreeNode(None, None, data[\"10\"].unique()[0])\n",
    "    else:\n",
    "        featureNum, threshold = find_best_feature(data)\n",
    "        rightBranch = data[(data[featureNum] >= threshold)]\n",
    "        leftBranch = data[(data[featureNum] < threshold)]\n",
    "        DecisionTreeClassifier = TreeNode(threshold, featureNum, None)\n",
    "        DecisionTreeClassifier.right = learn_tree(rightBranch)\n",
    "        DecisionTreeClassifier.left = learn_tree(leftBranch)\n",
    "\n",
    "        return DecisionTreeClassifier\n",
    "\n",
    "\n",
    "def is_pure(data: pd.Series) -> bool:\n",
    "    return len(pd.unique(data[\"10\"])) == 1\n",
    "\n",
    "\n",
    "def find_best_feature(data: pd.DataFrame) -> List:\n",
    "    features = data.drop([\"10\"], axis=1).columns\n",
    "    purityList = [find_best_threshold(data, feature) for feature in features]\n",
    "    _, threshold, featureNumber = sorted(purityList, key=lambda x: (x[0], x[1], int(x[2])), reverse=False)[0]\n",
    "    return [str(featureNumber), threshold]\n",
    "\n",
    "\n",
    "def find_best_threshold(data: pd.DataFrame, feature: str) -> List[float]:\n",
    "    featureList = []\n",
    "    for threshold in generate_threshold(data[feature]):\n",
    "        rightBran = data[\"10\"][(data[feature] >= threshold)]\n",
    "        leftBran = data[\"10\"][(data[feature] < threshold)]\n",
    "        weight = rightBran.count() / (rightBran.count() + leftBran.count())\n",
    "        impurity = weight * gini_coef(rightBran) + (1 - weight) * gini_coef(leftBran)\n",
    "        featureList.append([impurity, threshold, feature])\n",
    "\n",
    "    return sorted(featureList, key=lambda x: (x[0], x[1], x[2]), reverse=False)[0]\n",
    "\n",
    "\n",
    "def generate_threshold(data: pd.Series) -> List[float]:\n",
    "    tmp = sorted(data.to_list())\n",
    "    thresholdList = []\n",
    "    for i in range(len(tmp)):\n",
    "        if i == (len(tmp) - 1):\n",
    "            break\n",
    "        thresholdList.append((tmp[i] + tmp[i + 1]) / 2)\n",
    "\n",
    "    return list(set(thresholdList))\n",
    "\n",
    "\n",
    "def gini_coef(data: pd.Series) -> float:\n",
    "    probability = (data.value_counts() / len(data)).values\n",
    "    return 1 - np.sum(np.square(probability))\n",
    "\n",
    "\n",
    "def calculate_eout(classifier, testData: pd.DataFrame) -> float:\n",
    "    score = 0\n",
    "    for index, row in testData.iterrows():\n",
    "        predict = tree_predict(classifier, row[:10])\n",
    "        score += int(predict != row[10])\n",
    "\n",
    "    return score / testData.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eout of a decsiob tree is 0.20\n"
     ]
    }
   ],
   "source": [
    "tree = learn_tree(train)\n",
    "eout = calculate_eout(tree, test)\n",
    "print(f\"Eout of a decsiob tree is {eout:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_nums = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_with_bagging_eout(n: int, data: pd.DataFrame, test: pd.DataFrame) -> float:\n",
    "    def decision_tree(data: pd.DataFrame, test: pd.DataFrame) -> float:\n",
    "        tree = learn_tree(bagging(data))\n",
    "        return calculate_eout(tree, test)\n",
    "    \n",
    "    errorList = Parallel(n_jobs=cpu_nums)(delayed(decision_tree)(data, test) for _ in range(n))\n",
    "        \n",
    "    return sum(errorList) / len(errorList)\n",
    "        \n",
    "    \n",
    "def bagging(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    indexes = np.random.choice(data.index, size=int(0.5 * data.shape[0]), replace=True)\n",
    "    return data.iloc[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eout of 2000 trees is 0.24\n"
     ]
    }
   ],
   "source": [
    "eout = random_forest_with_bagging_eout(2000, train, test)\n",
    "print(f\"Eout of 2000 trees is {eout:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def random_forest_with_bagging_and_vote(n: int, data: pd.DataFrame, test: pd.DataFrame, error: str) -> float:\n",
    "    def decision_tree(data: pd.DataFrame) -> float:\n",
    "        return learn_tree(bagging(data))\n",
    "    \n",
    "    def random_tree_error(treeList: List[TreeNode], data: List[int]) -> int:\n",
    "        error = 0\n",
    "        for index, row in data.iterrows():\n",
    "            predictList = []\n",
    "            for tree in treeList:\n",
    "                while (not tree.value):\n",
    "                    if row[int(tree.feature)] >= tree.threshold:\n",
    "                        tree = tree.right\n",
    "                    else:\n",
    "                        tree = tree.left\n",
    "                predictList.append(tree.value)\n",
    "            \n",
    "            if sum(predictList) / len(predictList) >= 0:\n",
    "                error += (row[10] != 1)\n",
    "            else:\n",
    "                error += (row[10] != -1)\n",
    "        \n",
    "        return error / data.shape[0]\n",
    "    \n",
    "    treeList = Parallel(n_jobs=cpu_nums)(delayed(decision_tree)(data) for _ in range(n))\n",
    "    if error == \"ein\":\n",
    "        return random_tree_error(treeList, data)\n",
    "    elif error == \"eout\":\n",
    "        return random_tree_error(treeList, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ein of 2000 trees is 0.01\n"
     ]
    }
   ],
   "source": [
    "ein = random_forest_with_bagging_and_vote(2000, train, test, \"ein\")\n",
    "print(f\"Ein of 2000 trees is {ein:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eout of 2000 trees is 0.16\n"
     ]
    }
   ],
   "source": [
    "eout = random_forest_with_bagging_and_vote(2000, train, test, \"eout\")\n",
    "print(f\"Eout of 2000 trees is {eout:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 19\n",
    "\n",
    "Answer: (b)\n",
    "\n",
    "會最喜歡Matrix Factorization這個單元的原因是，之前有接觸過NN覺得NN可以某種程度上，做出feature extraction的效果，但是卻沒有想過NN居然可以將抽象的feature轉換成有意義的特徵，我想這堂課是讓我最驚訝的，更讓我知道推薦系統最初階的模型大概像這個樣子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 20\n",
    "\n",
    "Answer: (d)\n",
    "\n",
    "其實技法每個單元我都很喜歡，真的很難做抉擇，硬要選的話我會選Adaboost跟Gradient Boosting的模型，因為老師講蘋果的故事，太過精彩了且常常讓我印象深刻，作夢都在想XDD，讓我難以入睡。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
